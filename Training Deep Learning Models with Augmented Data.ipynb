{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hy4DrALWHM5A",
    "outputId": "baa09b7a-44c6-46d0-ce40-7fc72372a42e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "B_o7TnFmG4Fl"
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel('dataBerita(augmentasi).xlsx', usecols=['kategori', 'berita'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kategori</th>\n",
       "      <th>berita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>valid;</td>\n",
       "      <td>Gunung Agung erupsi untuk pertama kali pada 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>valid;</td>\n",
       "      <td>Jakarta CNN Indonesia -- Menteri BUMN Erick T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>valid;</td>\n",
       "      <td>Dosen Fakultas Kedokteran Hewan IPB Yusuf Ridw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>valid;</td>\n",
       "      <td>Jakarta - Dua anggota TNI Serda N dan Serda DA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>valid;</td>\n",
       "      <td>Akui Tembak Jatuh Pesawat Ukraina Iran Tuai Ge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  kategori                                             berita\n",
       "0   valid;   Gunung Agung erupsi untuk pertama kali pada 2...\n",
       "1   valid;   Jakarta CNN Indonesia -- Menteri BUMN Erick T...\n",
       "2   valid;  Dosen Fakultas Kedokteran Hewan IPB Yusuf Ridw...\n",
       "3   valid;  Jakarta - Dua anggota TNI Serda N dan Serda DA...\n",
       "4   valid;  Akui Tembak Jatuh Pesawat Ukraina Iran Tuai Ge..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['kategori', 'berita'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k0E2o2PxJf8y",
    "outputId": "7bd92f72-6ff3-4964-ac94-2ff646ae25fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['valid;' '\\nvalid;' 'valid; ' 'hoax;']\n"
     ]
    }
   ],
   "source": [
    "print(data['kategori'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Kl7m61jMJTaF"
   },
   "outputs": [],
   "source": [
    "# Membersihkan data dengan menghapus whitespace dan karakter tidak perlu\n",
    "data['kategori'] = data['kategori'].str.strip()        # Menghapus whitespace di awal dan akhir\n",
    "data['kategori'] = data['kategori'].str.replace(';', '', regex=False)  # Menghapus tanda semicolon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sIMlJvFdJngU",
    "outputId": "0eeeac1b-b9b1-4160-8ce4-ca64a6ec1406"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['valid' 'hoax']\n"
     ]
    }
   ],
   "source": [
    "print(data['kategori'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hoax samples: 432\n",
      "Number of valid samples: 432\n"
     ]
    }
   ],
   "source": [
    "label_counts = data['kategori'].value_counts()\n",
    "\n",
    "print(\"Number of hoax samples:\", label_counts.get('hoax', 0))\n",
    "print(\"Number of valid samples:\", label_counts.get('valid', 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['kategori_enc'] = data['kategori'].map({'valid': 0, 'hoax': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kategori</th>\n",
       "      <th>berita</th>\n",
       "      <th>kategori_enc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>valid</td>\n",
       "      <td>Gunung Agung erupsi untuk pertama kali pada 2...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>valid</td>\n",
       "      <td>Jakarta CNN Indonesia -- Menteri BUMN Erick T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>valid</td>\n",
       "      <td>Dosen Fakultas Kedokteran Hewan IPB Yusuf Ridw...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>valid</td>\n",
       "      <td>Jakarta - Dua anggota TNI Serda N dan Serda DA...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>valid</td>\n",
       "      <td>Akui Tembak Jatuh Pesawat Ukraina Iran Tuai Ge...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  kategori                                             berita  kategori_enc\n",
       "0    valid   Gunung Agung erupsi untuk pertama kali pada 2...             0\n",
       "1    valid   Jakarta CNN Indonesia -- Menteri BUMN Erick T...             0\n",
       "2    valid  Dosen Fakultas Kedokteran Hewan IPB Yusuf Ridw...             0\n",
       "3    valid  Jakarta - Dua anggota TNI Serda N dan Serda DA...             0\n",
       "4    valid  Akui Tembak Jatuh Pesawat Ukraina Iran Tuai Ge...             0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "qFmGT2c2IiaL"
   },
   "outputs": [],
   "source": [
    "x = data.drop('kategori_enc',axis=1)\n",
    "y = data['kategori_enc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alaz6thzrGXW"
   },
   "source": [
    "Data Pre-processing and One Hot Represenatation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words: 6641\n",
      "\n",
      "Top 20 most frequent words:\n",
      "orang: 460\n",
      "indonesia: 341\n",
      "presiden: 239\n",
      "jakarta: 237\n",
      "milik: 198\n",
      "anak: 197\n",
      "rumah: 196\n",
      "polisi: 195\n",
      "korban: 186\n",
      "negara: 183\n",
      "salah: 171\n",
      "jalan: 167\n",
      "hasil: 166\n",
      "bom: 161\n",
      "temu: 158\n",
      "masyarakat: 157\n",
      "perintah: 156\n",
      "tinggal: 156\n",
      "serang: 155\n",
      "lapor: 153\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    words = text.split()\n",
    "    return words\n",
    "\n",
    "def analyze_word_frequency(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Preprocess and count word frequencies\n",
    "    words = preprocess_text(text)\n",
    "    word_freq = Counter(words)\n",
    "    \n",
    "    # Sort frequencies in descending order\n",
    "    sorted_frequencies = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"Total unique words:\", len(word_freq))\n",
    "    print(\"\\nTop 20 most frequent words:\")\n",
    "    for word, freq in sorted_frequencies[:20]:\n",
    "        print(f\"{word}: {freq}\")\n",
    "    \n",
    "    return word_freq, sorted_frequencies\n",
    "\n",
    "# Usage\n",
    "file_path = 'corpus_Aug.txt'\n",
    "word_frequencies, sorted_freq = analyze_word_frequency(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the corpus from the text file\n",
    "corpus = []\n",
    "with open('corpus_Aug.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus = [line.strip() for line in f.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Panjang kalimat maksimum: 625\n",
      "Kalimat dengan panjang maksimum: liput com jakarta desah kokpit maskapai terbang lion air marak berita tanah air dunia sorot kutip news com au sabtu media australia tulis picu maskapai terbang murah besar indonesia lidi internal kopilot larang terbang tulis media negeri kanguru kutip nyata direktur lion air edward sirait nyata langkah cabut izin terbang kompensasi si kopilot aku cipta desah heboh kejut ulang muat media inggris daily mail situs berita tajuk artikel kait pilot banned from flying after offering divorced air hostess as compensation for a delay on a flight to bal media pakistan tribune pk artikel indonesian pilot grounded for offering hostess as compensation berita heboh lapor tumpang nama lambertus maengkom media pakistan beber pesawat lion air lepas landas lambat pulau jawa kopilot canda tawar pramugari janda tugas minta maaf keras suara situs singapura the straits times tinggal tulis desah kokpit lion air laman judul indonesia s lion air grounds pilot for offering flight attendant as compensation for delay media malaysia the star online tulis headline outrage over attendant offered for flight delay media asing selandia stuff co nz tajuk grounded pilot denies offering flight attendant to passenger eyewitneess news rengkuh ramai berita artikel judul pilot grounded for apparently offering hostess as compensation jakarta tumpang jt rute surabaya denpasar heran tingkah laku kru lion air jumpa terbang muncul malam sabtu november suara desah keras suara umum nilai janggal etis suara rupa desah umum layak tawar pramugari status janda tugas dengar ucap pramugari cantik tinggal suami gagal nikah pertama lambertus maenkom salah tumpang bincang liput com kamis tumpang akses microphone imbuh tingkah aneh kru lion air muncul pesawat lepas landas jadi ulang roda pesawat hasil darat bandara ngurah rai denpasar awak kabin pegumuman tumpang duduk tempat pesawat total henti pramugara pramugari arah tumpang ambil bagasi tunjuk tumpang tranasit umum transit suara kokpit langsung cekat timpal umum awak kabin transit kemane lambertus tiru suara keras suara kabin tumpang jt rute terbang surabaya denpasar terbang wib sabtu nobember kejut tatap mulut tumpang lion air heran nada keras suara dengar suara aneh desah desah speaker kabin jalan tulis lambertus laman bandara web id minggu november unggah wib suara desah jantung tumpang debar tumpang khawatir selamat jalan terima laku kru maskapai swasta nasional tarif rendah lambertus tumpang putus klarifikasi langsung pilot kopilot sampa ngurah rai denpasar daya pilot tanggung terbang tolak temu tumpang lambertus putus tulis kisah pahit nya laman bandara web id lambertus sepele pilot kopilot milik tanggung penuh jaga selamat tumpang salah tolok ukur disiplin kru tugas pesawat pilot disiplin bertanggungjawab nyawa nyawa orang bawa lambertus liput com rangkum jejak desah resah jantung tumpang dentum kronologi sabtu november wib pesawat nomor terbang jt jadwal lepas landas bandara juanda pesawat alami lambat terbang pemberitahuan rinci maskapai sebab lambat wib pesawat lepas landas terbang tumpang heran umum keras suara kokpit desah tawar pramugari janda tumpang sontak isi kursi tumpang kejut tatap delay jam sambut main main keluh lambertus wita roda pesawat darat mulus bandara ngurah rai pramugari umum tumpang suara kokpit timpal standar prosedur lion air take off landing momen krusial main nyawa tumpang tangan pilot lambertus nada tumpang geram tingkah laku pilot kopilot tunggu pesawat klarifikasi perihal lazim gagal jawab minggu november lambertus lampias jengkel maskapai tumpang malam tulis laman bandara web id rabu november lion air rampung lidi internal aku salah prosedur kopilot ban tawar janda pramugari tumpang hasil temu lion air langgar prosedur kopilot ucap selamat ulang salah awak kabin announcement tegas kopilot mabuk pengaruh narkoba berita sehat walafiat kuat saksi pilot in command awak kabin kait suara desah berita kopilot announcement napas kopilot sengal sengal bicara posisi mic bibir tarik nafas bicara dengar desah tun maskapai tarif rendah sanksi kopilot langgar prosedur announcement kait ucap ulang kopilot sanksi hukum terbang grounded direktur edward sirait terang tulis terima liput com\n",
      "Panjang maxlen yang direkomendasikan (95 persentil): 171\n"
     ]
    }
   ],
   "source": [
    "# Hitung panjang setiap kalimat\n",
    "sentence_lengths = [len(sentence.split()) for sentence in corpus]\n",
    "\n",
    "# Temukan panjang maksimum dan kalimat yang memiliki panjang maksimum tersebut\n",
    "max_length = max(sentence_lengths)\n",
    "max_length_index = sentence_lengths.index(max_length)\n",
    "longest_sentence = corpus[max_length_index]\n",
    "\n",
    "# Menghitung panjang setiap kalimat\n",
    "sentence_lengths = [len(sentence.split()) for sentence in corpus]\n",
    "\n",
    "# Menemukan panjang pada persentil ke-95\n",
    "maxlen_recommended = int(np.percentile(sentence_lengths, 95))\n",
    "\n",
    "print(\"Panjang kalimat maksimum:\", max_length)\n",
    "print(\"Kalimat dengan panjang maksimum:\", longest_sentence)\n",
    "print(\"Panjang maxlen yang direkomendasikan (95 persentil):\", maxlen_recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "CDziIFUcMiZC"
   },
   "outputs": [],
   "source": [
    "# We need to reset the indices as we have sampled our initial news dataset\n",
    "msg = x.copy()\n",
    "msg.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "3Y5MaaTjOR8V"
   },
   "outputs": [],
   "source": [
    "# import StemmerFactory class\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "collapsed": true,
    "id": "AVDHu8U_MjRq",
    "outputId": "6cac16c6-14dc-4ed4-9812-304b3c1f32e8"
   },
   "outputs": [],
   "source": [
    "#Data pre-pocessing\n",
    "\n",
    "corpus=[]\n",
    "for i in range(0,len(msg)):\n",
    "    review= re.sub('[^a-zA-Z]',' ',msg['berita'][i])\n",
    "    review= review.lower()\n",
    "    review= review.split()\n",
    "\n",
    "    review= [stemmer.stem(word) for word in review if not word in stopwords.words('indonesian')]\n",
    "    review= ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "Yx2D9yFOP8yT"
   },
   "outputs": [],
   "source": [
    "# Read the corpus from the text file\n",
    "corpus = []\n",
    "with open('corpus.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus = [line.strip() for line in f.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "864"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot frepresentation\n",
    "voc_size = 5500\n",
    "one_hot_re = [one_hot(element,voc_size) for element in corpus]\n",
    "\n",
    "#using pad_sequences to make all the representations of fixed length\n",
    "sentence_length=171\n",
    "embedded_docs= pad_sequences(one_hot_re, padding='pre', maxlen=sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "ikM9G6WYQFa6"
   },
   "outputs": [],
   "source": [
    "# Tokenize the corpus\n",
    "tokenizer = Tokenizer(num_words=6500)  # Use the same vocab size as during training\n",
    "tokenizer.fit_on_texts(corpus)  # Fit tokenizer on the new corpus (could use the same tokenizer as before)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "# Pad the sequences to ensure the same length as the training data\n",
    "sentence_length = 171  # Ensure this matches the training sentence length\n",
    "embedded_docs = pad_sequences(sequences, padding='pre', maxlen=sentence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXQ2iCDnrK0X"
   },
   "source": [
    "Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "MJBw9KasnUnt"
   },
   "outputs": [],
   "source": [
    "x_final = np.array(embedded_docs)\n",
    "y_final= np.array(y)\n",
    "x_train,x_test, y_train,y_test = train_test_split(x_final, y_final,test_size=0.33,random_state=42,stratify=y_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up k-fold cross-validation\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500\n",
      "100\n",
      "171\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_features = 100\n",
    "voc_size=6500\n",
    "sentence_length=171\n",
    "print(voc_size)\n",
    "print(embedding_vector_features)\n",
    "print(sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(voc_size, embedding_vector_features, sentence_length):\n",
    "    inputs = tf.keras.Input(shape=(sentence_length,))\n",
    "    embedding_layer = Embedding(voc_size, embedding_vector_features)(inputs)\n",
    "    lstm_layer = LSTM(32)(embedding_layer)\n",
    "    outputs = Dense(1, activation='sigmoid')(lstm_layer)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "Best Model Accuracy: 0.976878612716763\n",
      "Best Model Precision: 0.9743589743589743\n",
      "Best Model Recall: 0.9743589743589743\n",
      "Best Model F1-Score: 0.9743589743589743\n",
      "Best Model Validation Loss: [0.6327808499336243, 0.4997037351131439, 0.35079246759414673, 0.23313888907432556, 0.17349664866924286, 0.11576905101537704, 0.09121519327163696, 0.07915609329938889, 0.07426758855581284, 0.07154098898172379]\n",
      "Best Model Validation Accuracy: [0.8901734352111816, 0.849711000919342, 0.9190751314163208, 0.9537572264671326, 0.9595375657081604, 0.9710982441902161, 0.9826589822769165, 0.9826589822769165, 0.9826589822769165, 0.9768785834312439]\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0 \n",
    "best_precision = 0 \n",
    "best_recall = 0  \n",
    "best_f1 = 0\n",
    "\n",
    "best_model = None \n",
    "\n",
    "best_val_loss = None  \n",
    "best_val_accuracy = None  \n",
    "\n",
    "for train_index, test_index in kf.split(x_final):\n",
    "    x_train, x_test = x_final[train_index], x_final[test_index]\n",
    "    y_train, y_test = y_final[train_index], y_final[test_index]\n",
    "    \n",
    "    # Create a new model for each fold\n",
    "    model = create_model(voc_size=6500, embedding_vector_features=100, sentence_length=171)\n",
    "    \n",
    "    # Set up the model checkpoint to save the best model during training\n",
    "    checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min', verbose=0)\n",
    "    tensorboard_callback = TensorBoard(log_dir='./logs')\n",
    "\n",
    "    # Train the model on the current fold with validation data\n",
    "    history = model.fit(x_train, y_train, epochs=10, batch_size=64, verbose=0, validation_data=(x_test, y_test), callbacks=[checkpoint, tensorboard_callback])  \n",
    "    \n",
    "    # Load the best model based on validation loss\n",
    "    model = load_model('best_model.keras')\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = (model.predict(x_test) > 0.5).astype(\"int32\").flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Track the best model based on accuracy\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_precision = precision\n",
    "        best_recall = recall\n",
    "        best_f1 = f1\n",
    "\n",
    "        best_model = model\n",
    "        # Store the validation loss and accuracy for the best model\n",
    "        best_val_loss = history.history['val_loss']\n",
    "        best_val_accuracy = history.history['val_accuracy']\n",
    "    \n",
    "print(\"Best Model Accuracy:\", best_accuracy)\n",
    "print(\"Best Model Precision:\", best_precision)\n",
    "print(\"Best Model Recall:\", best_recall)\n",
    "print(\"Best Model F1-Score:\", best_f1)\n",
    "\n",
    "# After the K-fold cross-validation loop, print eval losses and accuracies for the best model\n",
    "print(\"Best Model Validation Loss:\", best_val_loss)\n",
    "print(\"Best Model Validation Accuracy:\", best_val_accuracy)\n",
    "\n",
    "# Optionally, save the best model\n",
    "best_model.save('t-100-6500-lstm.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "best_model = load_model('t-100-6500-lstm.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>berita</th>\n",
       "      <th>kategori</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hakim Wahyu Iman Santoso Alami Kecelakaan Tung...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MEGAWATI DAN PUAN BERMAIN SLOT Nenek lampir pe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JONATHAN LATUMAHINA SEORANG NASRANI DAN PENYUS...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PDI-P Diblacklist dari Peserta Pilpres, Tak Bi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Presiden Joe Biden dan Volodymyr Zelenskyy Ber...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              berita  kategori\n",
       "0  Hakim Wahyu Iman Santoso Alami Kecelakaan Tung...         1\n",
       "1  MEGAWATI DAN PUAN BERMAIN SLOT Nenek lampir pe...         1\n",
       "2  JONATHAN LATUMAHINA SEORANG NASRANI DAN PENYUS...         1\n",
       "3  PDI-P Diblacklist dari Peserta Pilpres, Tak Bi...         1\n",
       "4  Presiden Joe Biden dan Volodymyr Zelenskyy Ber...         1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the model with new data\n",
    "test = pd.read_excel('../test.xlsx', usecols=['kategori', 'berita'])\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print(test.shape)\n",
    "print(test['kategori'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hoax samples: 500\n",
      "Number of valid samples: 500\n"
     ]
    }
   ],
   "source": [
    "# Count occurrences of each label in the 'kategori' column\n",
    "label_counts = test['kategori'].value_counts()\n",
    "\n",
    "# Display the counts for \"hoax\" and \"valid\" (adjust if labels are different)\n",
    "print(\"Number of hoax samples:\", label_counts.get(1, 0))\n",
    "print(\"Number of valid samples:\", label_counts.get(0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target labels (0 for valid, 1 for hoax)\n",
    "test['kategori_enc'] = test['kategori']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Sastrawi stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# Preprocess text: cleaning, stemming, removing stopwords\n",
    "corpus_test = []\n",
    "for i in range(len(test)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', test['berita'][i])  # Remove non-letter characters\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [stemmer.stem(word) for word in review if word not in stopwords.words('indonesian')]\n",
    "    review = ' '.join(review)\n",
    "    corpus_test.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus saved as 'corpus_test.txt'\n"
     ]
    }
   ],
   "source": [
    "# Save the `corpus` to a text file\n",
    "with open('corpus_test.txt', 'w') as file:\n",
    "    for sentence in corpus_test:\n",
    "        file.write(sentence + '\\n')\n",
    "\n",
    "print(\"Corpus saved as 'corpus_test.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the corpus from the text file\n",
    "corpus_test = []\n",
    "with open('corpus_test.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus_test = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5500\n",
      "171\n"
     ]
    }
   ],
   "source": [
    "# Define necessary preprocessing parameters\n",
    "voc_size = 5500           # Must match training\n",
    "sentence_length = 171   # Must match training\n",
    "print(voc_size)\n",
    "print(sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Accuracy on new data: 0.725\n",
      "Precision on new data: 0.7764127764127764\n",
      "Recall on new data: 0.632\n",
      "F1 Score on new data: 0.6968026460859978\n"
     ]
    }
   ],
   "source": [
    "# Convert text to sequences using one-hot encoding and pad the sequences\n",
    "one_hot_re = [one_hot(sentence, voc_size) for sentence in corpus_test]\n",
    "new_x_final = pad_sequences(one_hot_re, padding='pre', maxlen=sentence_length)\n",
    "\n",
    "# Get the true labels (if available) for evaluation\n",
    "new_y_final = test['kategori_enc'].values\n",
    "# Predict with the new data\n",
    "new_predictions = (best_model.predict(new_x_final) > 0.5).astype(\"int32\").flatten()\n",
    "\n",
    "# Evaluate the new predictions (if labels are available)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "new_accuracy = accuracy_score(new_y_final, new_predictions)\n",
    "new_precision = precision_score(new_y_final, new_predictions)\n",
    "new_recall = recall_score(new_y_final, new_predictions)\n",
    "new_f1 = f1_score(new_y_final, new_predictions)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"Accuracy on new data: {new_accuracy}\")\n",
    "print(f\"Precision on new data: {new_precision}\")\n",
    "print(f\"Recall on new data: {new_recall}\")\n",
    "print(f\"F1 Score on new data: {new_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "Accuracy on new data: 0.729\n",
      "Precision on new data: 0.865814696485623\n",
      "Recall on new data: 0.542\n",
      "F1 Score on new data: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the corpus\n",
    "tokenizer = Tokenizer(num_words=6500)  # Use the same vocab size as during training\n",
    "tokenizer.fit_on_texts(corpus_test)  # Fit tokenizer on the new corpus (could use the same tokenizer as before)\n",
    "sequences = tokenizer.texts_to_sequences(corpus_test)\n",
    "\n",
    "# Pad the sequences to ensure the same length as the training data\n",
    "sentence_length = 171  # Ensure this matches the training sentence length\n",
    "embedded_docs = pad_sequences(sequences, padding='pre', maxlen=sentence_length)\n",
    "\n",
    "# Get the true labels (if available) for evaluation\n",
    "new_y_final = test['kategori_enc'].values  # Assuming 'kategori' column contains labels (0 or 1)\n",
    "\n",
    "# Predict with the model\n",
    "new_predictions = (best_model.predict(embedded_docs) > 0.5).astype(\"int32\").flatten()\n",
    "\n",
    "# Evaluate the new predictions (if labels are available)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "new_accuracy = accuracy_score(new_y_final, new_predictions)\n",
    "new_precision = precision_score(new_y_final, new_predictions)\n",
    "new_recall = recall_score(new_y_final, new_predictions)\n",
    "new_f1 = f1_score(new_y_final, new_predictions)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"Accuracy on new data: {new_accuracy}\")\n",
    "print(f\"Precision on new data: {new_precision}\")\n",
    "print(f\"Recall on new data: {new_recall}\")\n",
    "print(f\"F1 Score on new data: {new_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVzVhxtJoZyV"
   },
   "source": [
    "### Using Dropout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the corpus from the text file\n",
    "corpus = []\n",
    "with open('corpus_Aug.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus = [line.strip() for line in f.readlines()]\n",
    "\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500\n",
      "100\n",
      "171\n"
     ]
    }
   ],
   "source": [
    "voc_size=6500\n",
    "embedding_vector_features=100\n",
    "\n",
    "print(voc_size)\n",
    "print(embedding_vector_features)\n",
    "print(sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot frepresentation\n",
    "\n",
    "one_hot_re = [one_hot(element,voc_size) for element in corpus]\n",
    "\n",
    "#using pad_sequences to make all the representations of fixed length\n",
    "sentence_length=171\n",
    "embedded_docs= pad_sequences(one_hot_re, padding='pre', maxlen=sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the corpus\n",
    "tokenizer = Tokenizer(num_words=6500)  # Use the same vocab size as during training\n",
    "tokenizer.fit_on_texts(corpus)  # Fit tokenizer on the new corpus (could use the same tokenizer as before)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "# Pad the sequences to ensure the same length as the training data\n",
    "sentence_length = 171  # Ensure this matches the training sentence length\n",
    "embedded_docs = pad_sequences(sequences, padding='pre', maxlen=sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_final = np.array(embedded_docs)\n",
    "y_final= np.array(y)\n",
    "x_train,x_test, y_train,y_test = train_test_split(x_final, y_final,test_size=0.33,random_state=42,stratify=y_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(voc_size, embedding_vector_features, sentence_length):\n",
    "    inputs = tf.keras.Input(shape=(sentence_length,))\n",
    "    embedding_layer = Embedding(voc_size, embedding_vector_features)(inputs)\n",
    "    dropout_layer_1 = Dropout(0.2)(embedding_layer)\n",
    "\n",
    "    lstm_layer = LSTM(32)(dropout_layer_1)\n",
    "    dropout_layer_2 = Dropout(0.2)(lstm_layer)\n",
    "\n",
    "    outputs = Dense(1, activation='sigmoid')(dropout_layer_2)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Best Model Accuracy: 0.9826589595375722\n",
      "Best Model Precision: 0.9746835443037974\n",
      "Best Model Recall: 0.9871794871794872\n",
      "Best Model F1-Score: 0.9808917197452229\n",
      "Best Model Validation Loss: [0.6299266219139099, 0.480945348739624, 0.328502357006073, 0.2132781744003296, 0.15389706194400787, 0.1300736665725708, 0.11317156255245209, 0.09638959169387817, 0.07091765105724335, 0.06988024711608887]\n",
      "Best Model Validation Accuracy: [0.9190751314163208, 0.9017341136932373, 0.9248554706573486, 0.9421965479850769, 0.9479768872261047, 0.9653179049491882, 0.9653179049491882, 0.9710982441902161, 0.9826589822769165, 0.9826589822769165]\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0 \n",
    "best_precision = 0 \n",
    "best_recall = 0  \n",
    "best_f1 = 0\n",
    "\n",
    "best_model = None \n",
    "\n",
    "best_val_loss = None  \n",
    "best_val_accuracy = None  \n",
    "\n",
    "for train_index, test_index in kf.split(x_final):\n",
    "    x_train, x_test = x_final[train_index], x_final[test_index]\n",
    "    y_train, y_test = y_final[train_index], y_final[test_index]\n",
    "    \n",
    "    # Create a new model for each fold\n",
    "    model = create_model(voc_size=6500, embedding_vector_features=100, sentence_length=171)\n",
    "    \n",
    "    # Set up the model checkpoint to save the best model during training\n",
    "    checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min', verbose=0)\n",
    "    tensorboard_callback = TensorBoard(log_dir='./logs')\n",
    "\n",
    "    # Train the model on the current fold with validation data\n",
    "    history = model.fit(x_train, y_train, epochs=10, batch_size=64, verbose=0, validation_data=(x_test, y_test), callbacks=[checkpoint, tensorboard_callback])  \n",
    "    \n",
    "    # Load the best model based on validation loss\n",
    "    model = load_model('best_model.keras')\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = (model.predict(x_test) > 0.5).astype(\"int32\").flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Track the best model based on accuracy\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_precision = precision\n",
    "        best_recall = recall\n",
    "        best_f1 = f1\n",
    "\n",
    "        best_model = model\n",
    "        # Store the validation loss and accuracy for the best model\n",
    "        best_val_loss = history.history['val_loss']\n",
    "        best_val_accuracy = history.history['val_accuracy']\n",
    "    \n",
    "print(\"Best Model Accuracy:\", best_accuracy)\n",
    "print(\"Best Model Precision:\", best_precision)\n",
    "print(\"Best Model Recall:\", best_recall)\n",
    "print(\"Best Model F1-Score:\", best_f1)\n",
    "\n",
    "# After the K-fold cross-validation loop, print eval losses and accuracies for the best model\n",
    "print(\"Best Model Validation Loss:\", best_val_loss)\n",
    "print(\"Best Model Validation Accuracy:\", best_val_accuracy)\n",
    "\n",
    "# Optionally, save the best model\n",
    "best_model.save('t-100-6500-lstm_w_d.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model 828387478\n",
    "best_model = load_model('t-100-6500-lstm_w_d.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the corpus from the text file\n",
    "corpus_test = []\n",
    "with open('corpus_test.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus_test = [line.strip() for line in f.readlines()]\n",
    "\n",
    "len(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6500\n"
     ]
    }
   ],
   "source": [
    "print(voc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "Accuracy on new data: 0.743\n",
      "Precision on new data: 0.8461538461538461\n",
      "Recall on new data: 0.594\n",
      "F1 Score on new data: 0.6980023501762632\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the corpus\n",
    "tokenizer = Tokenizer(num_words=6500)  # Use the same vocab size as during training\n",
    "tokenizer.fit_on_texts(corpus_test)  # Fit tokenizer on the new corpus (could use the same tokenizer as before)\n",
    "sequences = tokenizer.texts_to_sequences(corpus_test)\n",
    "\n",
    "# Pad the sequences to ensure the same length as the training data\n",
    "sentence_length = 171  # Ensure this matches the training sentence length\n",
    "embedded_docs = pad_sequences(sequences, padding='pre', maxlen=sentence_length)\n",
    "\n",
    "# Get the true labels (if available) for evaluation\n",
    "new_y_final = test['kategori_enc'].values  # Assuming 'kategori' column contains labels (0 or 1)\n",
    "\n",
    "# Predict with the model\n",
    "new_predictions = (best_model.predict(embedded_docs) > 0.5).astype(\"int32\").flatten()\n",
    "\n",
    "# Evaluate the new predictions (if labels are available)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "new_accuracy = accuracy_score(new_y_final, new_predictions)\n",
    "new_precision = precision_score(new_y_final, new_predictions)\n",
    "new_recall = recall_score(new_y_final, new_predictions)\n",
    "new_f1 = f1_score(new_y_final, new_predictions)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"Accuracy on new data: {new_accuracy}\")\n",
    "print(f\"Precision on new data: {new_precision}\")\n",
    "print(f\"Recall on new data: {new_recall}\")\n",
    "print(f\"F1 Score on new data: {new_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "Accuracy on new data: 0.72\n",
      "Precision on new data: 0.7370689655172413\n",
      "Recall on new data: 0.684\n",
      "F1 Score on new data: 0.7095435684647303\n"
     ]
    }
   ],
   "source": [
    "voc_size=6500\n",
    "sentence_length=171\n",
    "\n",
    "# Convert text to sequences using one-hot encoding and pad the sequences\n",
    "one_hot_re = [one_hot(sentence, voc_size) for sentence in corpus_test]\n",
    "new_x_final = pad_sequences(one_hot_re, padding='pre', maxlen=sentence_length)\n",
    "\n",
    "# Prepare the labels\n",
    "new_y_final = test['kategori_enc'].values\n",
    "\n",
    "# Predict with the new data\n",
    "new_predictions = (best_model.predict(new_x_final) > 0.5).astype(\"int32\").flatten()\n",
    "\n",
    "# Evaluate the new predictions\n",
    "new_accuracy = accuracy_score(new_y_final, new_predictions)\n",
    "new_precision = precision_score(new_y_final, new_predictions)\n",
    "new_recall = recall_score(new_y_final, new_predictions)\n",
    "new_f1 = f1_score(new_y_final, new_predictions)\n",
    "\n",
    "print(f\"Accuracy on new data: {new_accuracy}\")\n",
    "print(f\"Precision on new data: {new_precision}\")\n",
    "print(f\"Recall on new data: {new_recall}\")\n",
    "print(f\"F1 Score on new data: {new_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQ04kvYaqxSl"
   },
   "source": [
    "### Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the corpus from the text file\n",
    "corpus = []\n",
    "with open('corpus.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "864\n",
      "6500\n",
      "171\n"
     ]
    }
   ],
   "source": [
    "voc_size = 6500\n",
    "embedding_vector_features = 100\n",
    "print(len(corpus))\n",
    "print(voc_size)\n",
    "print(sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot frepresentation\n",
    "\n",
    "one_hot_re = [one_hot(element,voc_size) for element in corpus]\n",
    "\n",
    "#using pad_sequences to make all the representations of fixed length\n",
    "sentence_length=171\n",
    "embedded_docs= pad_sequences(one_hot_re, padding='pre', maxlen=sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the corpus\n",
    "tokenizer = Tokenizer(num_words=6500)  # Use the same vocab size as during training\n",
    "tokenizer.fit_on_texts(corpus)  # Fit tokenizer on the new corpus (could use the same tokenizer as before)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "# Pad the sequences to ensure the same length as the training data\n",
    "sentence_length = 171  # Ensure this matches the training sentence length\n",
    "embedded_docs = pad_sequences(sequences, padding='pre', maxlen=sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_final = np.array(embedded_docs)\n",
    "y_final= np.array(y)\n",
    "x_train,x_test, y_train,y_test = train_test_split(x_final, y_final,test_size=0.33,random_state=42,stratify=y_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(voc_size, embedding_vector_features, sentence_length):\n",
    "    inputs = tf.keras.Input(shape=(sentence_length,))\n",
    "    embedding_layer = Embedding(voc_size, embedding_vector_features)(inputs)\n",
    "    bi_layer = Bidirectional(LSTM(32))(embedding_layer)\n",
    "\n",
    "    outputs = Dense(1, activation='sigmoid')(bi_layer)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step\n",
      "Best Model Accuracy: 0.9826589595375722\n",
      "Best Model Precision: 0.9795918367346939\n",
      "Best Model Recall: 0.9896907216494846\n",
      "Best Model F1-Score: 0.9846153846153847\n",
      "Best Model Validation Loss: [0.6757660508155823, 0.5285816788673401, 0.3544233441352844, 0.21315355598926544, 0.13982413709163666, 0.12155063450336456, 0.11289583146572113, 0.10612671077251434, 0.09786263108253479, 0.11741504818201065]\n",
      "Best Model Validation Accuracy: [0.6589595079421997, 0.9248554706573486, 0.9075144529342651, 0.9653179049491882, 0.9826589822769165, 0.9826589822769165, 0.9826589822769165, 0.9826589822769165, 0.9826589822769165, 0.9710982441902161]\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0 \n",
    "best_precision = 0 \n",
    "best_recall = 0  \n",
    "best_f1 = 0\n",
    "\n",
    "best_model = None \n",
    "\n",
    "best_val_loss = None  \n",
    "best_val_accuracy = None  \n",
    "\n",
    "for train_index, test_index in kf.split(x_final):\n",
    "    x_train, x_test = x_final[train_index], x_final[test_index]\n",
    "    y_train, y_test = y_final[train_index], y_final[test_index]\n",
    "    \n",
    "    # Create a new model for each fold\n",
    "    model = create_model(voc_size=6500, embedding_vector_features=100, sentence_length=171)\n",
    "    \n",
    "    # Set up the model checkpoint to save the best model during training\n",
    "    checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min', verbose=0)\n",
    "    tensorboard_callback = TensorBoard(log_dir='./logs')\n",
    "\n",
    "    # Train the model on the current fold with validation data\n",
    "    history = model.fit(x_train, y_train, epochs=10, batch_size=64, verbose=0, validation_data=(x_test, y_test), callbacks=[checkpoint, tensorboard_callback])  \n",
    "    \n",
    "    # Load the best model based on validation loss\n",
    "    model = load_model('best_model.keras')\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = (model.predict(x_test) > 0.5).astype(\"int32\").flatten()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Track the best model based on accuracy\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_precision = precision\n",
    "        best_recall = recall\n",
    "        best_f1 = f1\n",
    "\n",
    "        best_model = model\n",
    "        # Store the validation loss and accuracy for the best model\n",
    "        best_val_loss = history.history['val_loss']\n",
    "        best_val_accuracy = history.history['val_accuracy']\n",
    "    \n",
    "print(\"Best Model Accuracy:\", best_accuracy)\n",
    "print(\"Best Model Precision:\", best_precision)\n",
    "print(\"Best Model Recall:\", best_recall)\n",
    "print(\"Best Model F1-Score:\", best_f1)\n",
    "\n",
    "# After the K-fold cross-validation loop, print eval losses and accuracies for the best model\n",
    "print(\"Best Model Validation Loss:\", best_val_loss)\n",
    "print(\"Best Model Validation Accuracy:\", best_val_accuracy)\n",
    "\n",
    "# Optionally, save the best model\n",
    "best_model.save('t-100-6500-bilstm.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model('t-100-6500-bilstm.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the corpus from the text file\n",
    "corpus_test = []\n",
    "with open('corpus_test.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus_test = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "Accuracy on new data: 0.723\n",
      "Precision on new data: 0.8517350157728707\n",
      "Recall on new data: 0.54\n",
      "F1 Score on new data: 0.6609547123623011\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the corpus\n",
    "tokenizer = Tokenizer(num_words=6500)  # Use the same vocab size as during training\n",
    "tokenizer.fit_on_texts(corpus_test)  # Fit tokenizer on the new corpus (could use the same tokenizer as before)\n",
    "sequences = tokenizer.texts_to_sequences(corpus_test)\n",
    "\n",
    "# Pad the sequences to ensure the same length as the training data\n",
    "sentence_length = 171  # Ensure this matches the training sentence length\n",
    "embedded_docs = pad_sequences(sequences, padding='pre', maxlen=sentence_length)\n",
    "\n",
    "# Get the true labels (if available) for evaluation\n",
    "new_y_final = test['kategori_enc'].values  # Assuming 'kategori' column contains labels (0 or 1)\n",
    "\n",
    "# Predict with the model\n",
    "new_predictions = (best_model.predict(embedded_docs) > 0.5).astype(\"int32\").flatten()\n",
    "\n",
    "# Evaluate the new predictions (if labels are available)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "new_accuracy = accuracy_score(new_y_final, new_predictions)\n",
    "new_precision = precision_score(new_y_final, new_predictions)\n",
    "new_recall = recall_score(new_y_final, new_predictions)\n",
    "new_f1 = f1_score(new_y_final, new_predictions)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"Accuracy on new data: {new_accuracy}\")\n",
    "print(f\"Precision on new data: {new_precision}\")\n",
    "print(f\"Recall on new data: {new_recall}\")\n",
    "print(f\"F1 Score on new data: {new_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "Accuracy on new data: 0.692\n",
      "Precision on new data: 0.6927710843373494\n",
      "Recall on new data: 0.69\n",
      "F1 Score on new data: 0.6913827655310621\n"
     ]
    }
   ],
   "source": [
    "voc_size=6500\n",
    "sentence_length=171\n",
    "\n",
    "# Convert text to sequences using one-hot encoding and pad the sequences\n",
    "one_hot_re = [one_hot(sentence, voc_size) for sentence in corpus_test]\n",
    "new_x_final = pad_sequences(one_hot_re, padding='pre', maxlen=sentence_length)\n",
    "\n",
    "# Prepare the labels\n",
    "new_y_final = test['kategori_enc'].values\n",
    "\n",
    "# Predict with the new data\n",
    "new_predictions = (best_model.predict(new_x_final) > 0.5).astype(\"int32\").flatten()\n",
    "\n",
    "# Evaluate the new predictions\n",
    "new_accuracy = accuracy_score(new_y_final, new_predictions)\n",
    "new_precision = precision_score(new_y_final, new_predictions)\n",
    "new_recall = recall_score(new_y_final, new_predictions)\n",
    "new_f1 = f1_score(new_y_final, new_predictions)\n",
    "\n",
    "print(f\"Accuracy on new data: {new_accuracy}\")\n",
    "print(f\"Precision on new data: {new_precision}\")\n",
    "print(f\"Recall on new data: {new_recall}\")\n",
    "print(f\"F1 Score on new data: {new_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\Downloads\\Code Thesis\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1424 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nilai panjang teks pada persentil ke-95: 1031.5499999999997\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_excel('dataBerita.xlsx', usecols=['kategori', 'berita'])\n",
    "\n",
    "# Membersihkan data dengan menghapus whitespace dan karakter tidak perlu\n",
    "data['kategori'] = data['kategori'].str.strip()        # Menghapus whitespace di awal dan akhir\n",
    "data['kategori'] = data['kategori'].str.replace(';', '', regex=False)  # Menghapus tanda semicolon\n",
    "data['kategori_enc'] = data['kategori'].map({'valid': 0, 'hoax': 1})\n",
    "texts = data['berita'].tolist()\n",
    "\n",
    "# Initialize RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Tokenisasi dan hitung panjang masing-masing teks\n",
    "lengths = [len(tokenizer.encode(text)) for text in texts]\n",
    "\n",
    "# Hitung panjang pada persentil ke-95\n",
    "percentile_95 = np.percentile(lengths, 95)\n",
    "\n",
    "# Menampilkan hasil\n",
    "print(f\"Nilai panjang teks pada persentil ke-95: {percentile_95}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\Downloads\\Code Thesis\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\Downloads\\Code Thesis\\env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_excel('dataBerita.xlsx', usecols=['kategori', 'berita'])\n",
    "# Membersihkan data dengan menghapus whitespace dan karakter tidak perlu\n",
    "data['kategori'] = data['kategori'].str.strip()        # Menghapus whitespace di awal dan akhir\n",
    "data['kategori'] = data['kategori'].str.replace(';', '', regex=False)  # Menghapus tanda semicolon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['kategori_enc'] = data['kategori'].map({'valid': 0, 'hoax': 1})\n",
    "texts = data['berita'].tolist()\n",
    "labels = data['kategori_enc'].tolist()\n",
    "\n",
    "# Initialize RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['kategori_enc'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class for RoBERTa\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=216):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "# Define K-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "best_recall = 0\n",
    "best_model = None\n",
    "accuracy_scores, precision_scores, recall_scores, f1_scores = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Lenovo\\Downloads\\Code Thesis\\env\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_18472\\1246478591.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      " 10%|█         | 44/440 [03:20<13:08,  1.99s/it] \n",
      " 10%|█         | 44/440 [03:28<13:08,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3351345658302307, 'eval_accuracy': 0.896551724137931, 'eval_precision': 0.8947368421052632, 'eval_recall': 0.8717948717948718, 'eval_f1': 0.8831168831168831, 'eval_runtime': 7.417, 'eval_samples_per_second': 11.73, 'eval_steps_per_second': 1.483, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 88/440 [05:20<12:25,  2.12s/it]\n",
      " 20%|██        | 88/440 [05:28<12:25,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4350961148738861, 'eval_accuracy': 0.9080459770114943, 'eval_precision': 0.9428571428571428, 'eval_recall': 0.8461538461538461, 'eval_f1': 0.8918918918918919, 'eval_runtime': 7.7866, 'eval_samples_per_second': 11.173, 'eval_steps_per_second': 1.413, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 132/440 [07:36<13:11,  2.57s/it]\n",
      " 30%|███       | 132/440 [07:46<13:11,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.287454217672348, 'eval_accuracy': 0.9195402298850575, 'eval_precision': 0.8809523809523809, 'eval_recall': 0.9487179487179487, 'eval_f1': 0.9135802469135802, 'eval_runtime': 10.1626, 'eval_samples_per_second': 8.561, 'eval_steps_per_second': 1.082, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 176/440 [10:03<10:51,  2.47s/it]\n",
      " 40%|████      | 176/440 [10:11<10:51,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47007954120635986, 'eval_accuracy': 0.9195402298850575, 'eval_precision': 0.8636363636363636, 'eval_recall': 0.9743589743589743, 'eval_f1': 0.9156626506024096, 'eval_runtime': 8.7989, 'eval_samples_per_second': 9.888, 'eval_steps_per_second': 1.25, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 220/440 [12:21<08:10,  2.23s/it]\n",
      " 50%|█████     | 220/440 [12:30<08:10,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5012206435203552, 'eval_accuracy': 0.896551724137931, 'eval_precision': 0.875, 'eval_recall': 0.8974358974358975, 'eval_f1': 0.8860759493670886, 'eval_runtime': 8.1871, 'eval_samples_per_second': 10.627, 'eval_steps_per_second': 1.344, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 264/440 [14:34<06:35,  2.25s/it]\n",
      " 60%|██████    | 264/440 [14:42<06:35,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.39752474427223206, 'eval_accuracy': 0.9195402298850575, 'eval_precision': 0.9444444444444444, 'eval_recall': 0.8717948717948718, 'eval_f1': 0.9066666666666666, 'eval_runtime': 8.17, 'eval_samples_per_second': 10.649, 'eval_steps_per_second': 1.346, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 308/440 [16:40<04:34,  2.08s/it]\n",
      " 70%|███████   | 308/440 [16:48<04:34,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3905560374259949, 'eval_accuracy': 0.9425287356321839, 'eval_precision': 0.925, 'eval_recall': 0.9487179487179487, 'eval_f1': 0.9367088607594937, 'eval_runtime': 7.568, 'eval_samples_per_second': 11.496, 'eval_steps_per_second': 1.453, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 352/440 [18:46<03:03,  2.08s/it]\n",
      " 80%|████████  | 352/440 [18:53<03:03,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.38352400064468384, 'eval_accuracy': 0.9425287356321839, 'eval_precision': 0.925, 'eval_recall': 0.9487179487179487, 'eval_f1': 0.9367088607594937, 'eval_runtime': 7.5461, 'eval_samples_per_second': 11.529, 'eval_steps_per_second': 1.458, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 396/440 [20:50<01:33,  2.12s/it]\n",
      " 90%|█████████ | 396/440 [20:57<01:33,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.37745147943496704, 'eval_accuracy': 0.9425287356321839, 'eval_precision': 0.925, 'eval_recall': 0.9487179487179487, 'eval_f1': 0.9367088607594937, 'eval_runtime': 7.5665, 'eval_samples_per_second': 11.498, 'eval_steps_per_second': 1.454, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 440/440 [22:54<00:00,  2.08s/it]\n",
      "100%|██████████| 440/440 [23:03<00:00,  3.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.37454351782798767, 'eval_accuracy': 0.9425287356321839, 'eval_precision': 0.925, 'eval_recall': 0.9487179487179487, 'eval_f1': 0.9367088607594937, 'eval_runtime': 7.7089, 'eval_samples_per_second': 11.286, 'eval_steps_per_second': 1.427, 'epoch': 10.0}\n",
      "{'train_runtime': 1383.4786, 'train_samples_per_second': 2.494, 'train_steps_per_second': 0.318, 'train_loss': 0.12468470660122959, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:06<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Lenovo\\Downloads\\Code Thesis\\env\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_18472\\1246478591.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      " 10%|█         | 44/440 [01:56<13:56,  2.11s/it]\n",
      " 10%|█         | 44/440 [02:04<13:56,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.40745118260383606, 'eval_accuracy': 0.8505747126436781, 'eval_precision': 0.96875, 'eval_recall': 0.7209302325581395, 'eval_f1': 0.8266666666666667, 'eval_runtime': 7.5997, 'eval_samples_per_second': 11.448, 'eval_steps_per_second': 1.447, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 88/440 [04:01<12:22,  2.11s/it]\n",
      " 20%|██        | 88/440 [04:09<12:22,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3180750906467438, 'eval_accuracy': 0.8850574712643678, 'eval_precision': 0.9459459459459459, 'eval_recall': 0.813953488372093, 'eval_f1': 0.875, 'eval_runtime': 7.769, 'eval_samples_per_second': 11.198, 'eval_steps_per_second': 1.416, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 132/440 [06:05<10:38,  2.07s/it]\n",
      " 30%|███       | 132/440 [06:13<10:38,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3564257025718689, 'eval_accuracy': 0.9080459770114943, 'eval_precision': 0.9487179487179487, 'eval_recall': 0.8604651162790697, 'eval_f1': 0.9024390243902439, 'eval_runtime': 7.7009, 'eval_samples_per_second': 11.297, 'eval_steps_per_second': 1.428, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 176/440 [08:09<09:19,  2.12s/it]\n",
      " 40%|████      | 176/440 [08:17<09:19,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22682470083236694, 'eval_accuracy': 0.9310344827586207, 'eval_precision': 0.9512195121951219, 'eval_recall': 0.9069767441860465, 'eval_f1': 0.9285714285714286, 'eval_runtime': 7.707, 'eval_samples_per_second': 11.288, 'eval_steps_per_second': 1.427, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 220/440 [10:16<07:53,  2.15s/it]\n",
      " 50%|█████     | 220/440 [10:23<07:53,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.557304322719574, 'eval_accuracy': 0.896551724137931, 'eval_precision': 0.9722222222222222, 'eval_recall': 0.813953488372093, 'eval_f1': 0.8860759493670886, 'eval_runtime': 7.7549, 'eval_samples_per_second': 11.219, 'eval_steps_per_second': 1.418, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 264/440 [12:21<06:08,  2.09s/it]\n",
      " 60%|██████    | 264/440 [12:28<06:08,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.29005157947540283, 'eval_accuracy': 0.9540229885057471, 'eval_precision': 0.9534883720930233, 'eval_recall': 0.9534883720930233, 'eval_f1': 0.9534883720930233, 'eval_runtime': 7.6922, 'eval_samples_per_second': 11.31, 'eval_steps_per_second': 1.43, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 308/440 [14:26<04:34,  2.08s/it]\n",
      " 70%|███████   | 308/440 [14:33<04:34,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3808920085430145, 'eval_accuracy': 0.9425287356321839, 'eval_precision': 0.9318181818181818, 'eval_recall': 0.9534883720930233, 'eval_f1': 0.9425287356321839, 'eval_runtime': 7.6258, 'eval_samples_per_second': 11.409, 'eval_steps_per_second': 1.442, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 352/440 [16:29<03:06,  2.12s/it]\n",
      " 80%|████████  | 352/440 [16:36<03:06,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.372263103723526, 'eval_accuracy': 0.9425287356321839, 'eval_precision': 0.9318181818181818, 'eval_recall': 0.9534883720930233, 'eval_f1': 0.9425287356321839, 'eval_runtime': 7.625, 'eval_samples_per_second': 11.41, 'eval_steps_per_second': 1.443, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 396/440 [18:34<01:33,  2.13s/it]\n",
      " 90%|█████████ | 396/440 [18:42<01:33,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3497253954410553, 'eval_accuracy': 0.9540229885057471, 'eval_precision': 0.9534883720930233, 'eval_recall': 0.9534883720930233, 'eval_f1': 0.9534883720930233, 'eval_runtime': 7.6751, 'eval_samples_per_second': 11.335, 'eval_steps_per_second': 1.433, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 440/440 [20:39<00:00,  2.12s/it]\n",
      "100%|██████████| 440/440 [20:48<00:00,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.34802156686782837, 'eval_accuracy': 0.9540229885057471, 'eval_precision': 0.9534883720930233, 'eval_recall': 0.9534883720930233, 'eval_f1': 0.9534883720930233, 'eval_runtime': 7.6847, 'eval_samples_per_second': 11.321, 'eval_steps_per_second': 1.431, 'epoch': 10.0}\n",
      "{'train_runtime': 1248.0394, 'train_samples_per_second': 2.764, 'train_steps_per_second': 0.353, 'train_loss': 0.19932674061168323, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:06<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Lenovo\\Downloads\\Code Thesis\\env\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_18472\\1246478591.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      " 10%|█         | 44/440 [01:55<14:22,  2.18s/it]\n",
      " 10%|█         | 44/440 [02:03<14:22,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6965568661689758, 'eval_accuracy': 0.5697674418604651, 'eval_precision': 0.5647058823529412, 'eval_recall': 1.0, 'eval_f1': 0.7218045112781954, 'eval_runtime': 7.5831, 'eval_samples_per_second': 11.341, 'eval_steps_per_second': 1.451, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 88/440 [04:00<12:45,  2.17s/it]\n",
      " 20%|██        | 88/440 [04:08<12:45,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7294079065322876, 'eval_accuracy': 0.8488372093023255, 'eval_precision': 0.8431372549019608, 'eval_recall': 0.8958333333333334, 'eval_f1': 0.8686868686868687, 'eval_runtime': 7.3829, 'eval_samples_per_second': 11.649, 'eval_steps_per_second': 1.49, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 132/440 [06:05<11:13,  2.19s/it]\n",
      " 30%|███       | 132/440 [06:13<11:13,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6473089456558228, 'eval_accuracy': 0.872093023255814, 'eval_precision': 0.8936170212765957, 'eval_recall': 0.875, 'eval_f1': 0.8842105263157894, 'eval_runtime': 7.6249, 'eval_samples_per_second': 11.279, 'eval_steps_per_second': 1.443, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 176/440 [08:10<09:36,  2.18s/it]\n",
      " 40%|████      | 176/440 [08:18<09:36,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7999522089958191, 'eval_accuracy': 0.8604651162790697, 'eval_precision': 0.9090909090909091, 'eval_recall': 0.8333333333333334, 'eval_f1': 0.8695652173913043, 'eval_runtime': 7.4378, 'eval_samples_per_second': 11.563, 'eval_steps_per_second': 1.479, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 220/440 [10:15<07:57,  2.17s/it]\n",
      " 50%|█████     | 220/440 [10:22<07:57,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7661227583885193, 'eval_accuracy': 0.8837209302325582, 'eval_precision': 0.8958333333333334, 'eval_recall': 0.8958333333333334, 'eval_f1': 0.8958333333333334, 'eval_runtime': 7.5603, 'eval_samples_per_second': 11.375, 'eval_steps_per_second': 1.455, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 264/440 [12:21<06:28,  2.21s/it]\n",
      " 60%|██████    | 264/440 [12:28<06:28,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6808353066444397, 'eval_accuracy': 0.8837209302325582, 'eval_precision': 0.9130434782608695, 'eval_recall': 0.875, 'eval_f1': 0.8936170212765957, 'eval_runtime': 7.4116, 'eval_samples_per_second': 11.603, 'eval_steps_per_second': 1.484, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 308/440 [14:25<04:43,  2.15s/it]\n",
      " 70%|███████   | 308/440 [14:34<04:43,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.950936496257782, 'eval_accuracy': 0.8604651162790697, 'eval_precision': 0.86, 'eval_recall': 0.8958333333333334, 'eval_f1': 0.8775510204081632, 'eval_runtime': 8.8508, 'eval_samples_per_second': 9.717, 'eval_steps_per_second': 1.243, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 352/440 [17:04<03:59,  2.72s/it]\n",
      " 80%|████████  | 352/440 [17:14<03:59,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0405665636062622, 'eval_accuracy': 0.8604651162790697, 'eval_precision': 0.9090909090909091, 'eval_recall': 0.8333333333333334, 'eval_f1': 0.8695652173913043, 'eval_runtime': 10.1542, 'eval_samples_per_second': 8.469, 'eval_steps_per_second': 1.083, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 396/440 [19:42<02:02,  2.79s/it]\n",
      " 90%|█████████ | 396/440 [19:52<02:02,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1106778383255005, 'eval_accuracy': 0.8604651162790697, 'eval_precision': 0.9090909090909091, 'eval_recall': 0.8333333333333334, 'eval_f1': 0.8695652173913043, 'eval_runtime': 9.4445, 'eval_samples_per_second': 9.106, 'eval_steps_per_second': 1.165, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 440/440 [22:22<00:00,  2.83s/it]\n",
      "100%|██████████| 440/440 [22:33<00:00,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1503024101257324, 'eval_accuracy': 0.8604651162790697, 'eval_precision': 0.9090909090909091, 'eval_recall': 0.8333333333333334, 'eval_f1': 0.8695652173913043, 'eval_runtime': 9.4018, 'eval_samples_per_second': 9.147, 'eval_steps_per_second': 1.17, 'epoch': 10.0}\n",
      "{'train_runtime': 1353.0843, 'train_samples_per_second': 2.557, 'train_steps_per_second': 0.325, 'train_loss': 0.1492477763782848, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:09<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Lenovo\\Downloads\\Code Thesis\\env\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_18472\\1246478591.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      " 10%|█         | 44/440 [02:27<18:14,  2.76s/it]\n",
      " 10%|█         | 44/440 [02:36<18:14,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4098607301712036, 'eval_accuracy': 0.813953488372093, 'eval_precision': 1.0, 'eval_recall': 0.6190476190476191, 'eval_f1': 0.7647058823529411, 'eval_runtime': 9.1408, 'eval_samples_per_second': 9.408, 'eval_steps_per_second': 1.203, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 88/440 [05:04<16:19,  2.78s/it]\n",
      " 20%|██        | 88/440 [05:14<16:19,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28735974431037903, 'eval_accuracy': 0.872093023255814, 'eval_precision': 1.0, 'eval_recall': 0.7380952380952381, 'eval_f1': 0.8493150684931506, 'eval_runtime': 9.3238, 'eval_samples_per_second': 9.224, 'eval_steps_per_second': 1.18, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 132/440 [07:42<14:06,  2.75s/it]\n",
      " 30%|███       | 132/440 [07:52<14:06,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.12346210330724716, 'eval_accuracy': 0.9418604651162791, 'eval_precision': 1.0, 'eval_recall': 0.8809523809523809, 'eval_f1': 0.9367088607594937, 'eval_runtime': 9.8948, 'eval_samples_per_second': 8.691, 'eval_steps_per_second': 1.112, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 176/440 [10:19<12:03,  2.74s/it]\n",
      " 40%|████      | 176/440 [10:30<12:03,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.10693756490945816, 'eval_accuracy': 0.9767441860465116, 'eval_precision': 0.9545454545454546, 'eval_recall': 1.0, 'eval_f1': 0.9767441860465116, 'eval_runtime': 10.6125, 'eval_samples_per_second': 8.104, 'eval_steps_per_second': 1.037, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 220/440 [12:58<09:52,  2.69s/it]\n",
      " 50%|█████     | 220/440 [13:08<09:52,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22475533187389374, 'eval_accuracy': 0.9302325581395349, 'eval_precision': 0.95, 'eval_recall': 0.9047619047619048, 'eval_f1': 0.926829268292683, 'eval_runtime': 10.5014, 'eval_samples_per_second': 8.189, 'eval_steps_per_second': 1.047, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 264/440 [15:36<08:05,  2.76s/it]\n",
      " 60%|██████    | 264/440 [15:46<08:05,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.10589291155338287, 'eval_accuracy': 0.9767441860465116, 'eval_precision': 0.9761904761904762, 'eval_recall': 0.9761904761904762, 'eval_f1': 0.9761904761904762, 'eval_runtime': 10.0609, 'eval_samples_per_second': 8.548, 'eval_steps_per_second': 1.093, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 308/440 [18:13<06:00,  2.73s/it]\n",
      " 70%|███████   | 308/440 [18:23<06:00,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06396839767694473, 'eval_accuracy': 0.9883720930232558, 'eval_precision': 1.0, 'eval_recall': 0.9761904761904762, 'eval_f1': 0.9879518072289156, 'eval_runtime': 9.9924, 'eval_samples_per_second': 8.607, 'eval_steps_per_second': 1.101, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 352/440 [20:54<04:19,  2.95s/it]\n",
      " 80%|████████  | 352/440 [21:04<04:19,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1112014651298523, 'eval_accuracy': 0.9767441860465116, 'eval_precision': 1.0, 'eval_recall': 0.9523809523809523, 'eval_f1': 0.975609756097561, 'eval_runtime': 9.7918, 'eval_samples_per_second': 8.783, 'eval_steps_per_second': 1.123, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 396/440 [23:33<02:06,  2.87s/it]\n",
      " 90%|█████████ | 396/440 [23:43<02:06,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08177832514047623, 'eval_accuracy': 0.9883720930232558, 'eval_precision': 1.0, 'eval_recall': 0.9761904761904762, 'eval_f1': 0.9879518072289156, 'eval_runtime': 9.5806, 'eval_samples_per_second': 8.976, 'eval_steps_per_second': 1.148, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 440/440 [26:14<00:00,  2.90s/it]\n",
      "100%|██████████| 440/440 [26:25<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.09265393018722534, 'eval_accuracy': 0.9883720930232558, 'eval_precision': 1.0, 'eval_recall': 0.9761904761904762, 'eval_f1': 0.9879518072289156, 'eval_runtime': 10.0993, 'eval_samples_per_second': 8.515, 'eval_steps_per_second': 1.089, 'epoch': 10.0}\n",
      "{'train_runtime': 1585.577, 'train_samples_per_second': 2.182, 'train_steps_per_second': 0.278, 'train_loss': 0.1912561763416637, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:08<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Lenovo\\Downloads\\Code Thesis\\env\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_18472\\1246478591.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      " 10%|█         | 44/440 [02:30<18:42,  2.84s/it]\n",
      " 10%|█         | 44/440 [02:40<18:42,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.49206817150115967, 'eval_accuracy': 0.8604651162790697, 'eval_precision': 0.9705882352941176, 'eval_recall': 0.75, 'eval_f1': 0.8461538461538461, 'eval_runtime': 9.5083, 'eval_samples_per_second': 9.045, 'eval_steps_per_second': 1.157, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 88/440 [05:08<16:14,  2.77s/it]\n",
      " 20%|██        | 88/440 [05:19<16:14,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22673214972019196, 'eval_accuracy': 0.9534883720930233, 'eval_precision': 0.9761904761904762, 'eval_recall': 0.9318181818181818, 'eval_f1': 0.9534883720930233, 'eval_runtime': 10.5517, 'eval_samples_per_second': 8.15, 'eval_steps_per_second': 1.042, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 132/440 [07:46<14:00,  2.73s/it]\n",
      " 30%|███       | 132/440 [07:56<14:00,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5412914156913757, 'eval_accuracy': 0.8953488372093024, 'eval_precision': 0.972972972972973, 'eval_recall': 0.8181818181818182, 'eval_f1': 0.8888888888888888, 'eval_runtime': 10.1305, 'eval_samples_per_second': 8.489, 'eval_steps_per_second': 1.086, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 176/440 [10:06<09:30,  2.16s/it]\n",
      " 40%|████      | 176/440 [10:14<09:30,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3588232398033142, 'eval_accuracy': 0.9418604651162791, 'eval_precision': 0.975609756097561, 'eval_recall': 0.9090909090909091, 'eval_f1': 0.9411764705882353, 'eval_runtime': 7.8003, 'eval_samples_per_second': 11.025, 'eval_steps_per_second': 1.41, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 220/440 [12:09<07:52,  2.15s/it]\n",
      " 50%|█████     | 220/440 [12:17<07:52,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24453820288181305, 'eval_accuracy': 0.9651162790697675, 'eval_precision': 0.9767441860465116, 'eval_recall': 0.9545454545454546, 'eval_f1': 0.9655172413793104, 'eval_runtime': 7.472, 'eval_samples_per_second': 11.51, 'eval_steps_per_second': 1.472, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 264/440 [14:15<06:29,  2.21s/it]\n",
      " 60%|██████    | 264/440 [14:22<06:29,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.264659583568573, 'eval_accuracy': 0.9651162790697675, 'eval_precision': 0.9767441860465116, 'eval_recall': 0.9545454545454546, 'eval_f1': 0.9655172413793104, 'eval_runtime': 7.4078, 'eval_samples_per_second': 11.609, 'eval_steps_per_second': 1.485, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 308/440 [16:20<04:45,  2.16s/it]\n",
      " 70%|███████   | 308/440 [16:28<04:45,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26219823956489563, 'eval_accuracy': 0.9651162790697675, 'eval_precision': 0.9767441860465116, 'eval_recall': 0.9545454545454546, 'eval_f1': 0.9655172413793104, 'eval_runtime': 7.4519, 'eval_samples_per_second': 11.541, 'eval_steps_per_second': 1.476, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 352/440 [18:23<03:09,  2.15s/it]\n",
      " 80%|████████  | 352/440 [18:31<03:09,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3176257610321045, 'eval_accuracy': 0.9534883720930233, 'eval_precision': 0.9761904761904762, 'eval_recall': 0.9318181818181818, 'eval_f1': 0.9534883720930233, 'eval_runtime': 7.5669, 'eval_samples_per_second': 11.365, 'eval_steps_per_second': 1.454, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 396/440 [20:29<01:36,  2.18s/it]\n",
      " 90%|█████████ | 396/440 [20:36<01:36,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2882876992225647, 'eval_accuracy': 0.9651162790697675, 'eval_precision': 0.9767441860465116, 'eval_recall': 0.9545454545454546, 'eval_f1': 0.9655172413793104, 'eval_runtime': 7.5719, 'eval_samples_per_second': 11.358, 'eval_steps_per_second': 1.453, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 440/440 [22:33<00:00,  2.18s/it]\n",
      "100%|██████████| 440/440 [22:41<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3035212457180023, 'eval_accuracy': 0.9651162790697675, 'eval_precision': 0.9767441860465116, 'eval_recall': 0.9545454545454546, 'eval_f1': 0.9655172413793104, 'eval_runtime': 7.4242, 'eval_samples_per_second': 11.584, 'eval_steps_per_second': 1.482, 'epoch': 10.0}\n",
      "{'train_runtime': 1361.7328, 'train_samples_per_second': 2.541, 'train_steps_per_second': 0.323, 'train_loss': 0.14948257099498402, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:06<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kf.split(texts)):\n",
    "    print(f\"Training fold {fold + 1}...\")\n",
    "    \n",
    "    # Split the data for this fold\n",
    "    train_texts = [texts[i] for i in train_index]\n",
    "    test_texts = [texts[i] for i in test_index]\n",
    "    train_labels = [labels[i] for i in train_index]\n",
    "    test_labels = [labels[i] for i in test_index]\n",
    "    \n",
    "    # Create datasets for this fold\n",
    "    train_dataset = CustomDataset(train_texts, train_labels, tokenizer)\n",
    "    test_dataset = CustomDataset(test_texts, test_labels, tokenizer)\n",
    "    \n",
    "    # Load model for each fold\n",
    "    model = RobertaForSequenceClassification.from_pretrained('distilroberta-base', num_labels=2)\n",
    "    \n",
    "    # Define training arguments for each fold\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=10,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        logging_dir='./logs',\n",
    "        evaluation_strategy=\"epoch\"\n",
    "    )\n",
    "\n",
    "    # Define metrics function\n",
    "    def compute_metrics(pred):\n",
    "        labels = pred.label_ids\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "        accuracy = accuracy_score(labels, preds)\n",
    "        precision = precision_score(labels, preds)\n",
    "        recall = recall_score(labels, preds)\n",
    "        f1 = f1_score(labels, preds)\n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Train and evaluate\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    # Extract and store metrics for this fold\n",
    "    accuracy = eval_results['eval_accuracy']\n",
    "    precision = eval_results['eval_precision']\n",
    "    recall = eval_results['eval_recall']\n",
    "    f1 = eval_results['eval_f1']\n",
    "\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    # Track the best model based on accuracy\n",
    "    if recall > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.9364251982535864\n",
      "Average Precision: 0.9472070643582272\n",
      "Average Recall: 0.9230117427791846\n",
      "Average F1 Score: 0.9346613901595519\n",
      "Best model saved as 'transformer'\n"
     ]
    }
   ],
   "source": [
    "# Save the best model\n",
    "best_model.save_pretrained(\"transformer\")\n",
    "tokenizer.save_pretrained(\"transformer\")\n",
    "\n",
    "# Print average metrics across all folds\n",
    "print(f\"Average Accuracy: {np.mean(accuracy_scores)}\")\n",
    "print(f\"Average Precision: {np.mean(precision_scores)}\")\n",
    "print(f\"Average Recall: {np.mean(recall_scores)}\")\n",
    "print(f\"Average F1 Score: {np.mean(f1_scores)}\")\n",
    "print(\"Best model saved as 'transformer'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.785\n",
      "Precision: 0.7567567567567568\n",
      "Recall: 0.84\n",
      "F1 Score: 0.7962085308056872\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained model and tokenizer from the folder\n",
    "model = RobertaForSequenceClassification.from_pretrained('transformer')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('transformer')\n",
    "\n",
    "# Load the new dataset (e.g., 'test.xlsx')\n",
    "test_data = pd.read_excel('../test.xlsx')\n",
    "\n",
    "# Extract the true labels for the test set\n",
    "true_labels = test_data['kategori'].tolist()  # Assuming 'kategori_enc' has the true labels\n",
    "\n",
    "# Preprocess the new data (tokenize and prepare the input)\n",
    "test_texts = test_data['berita'].tolist()\n",
    "\n",
    "# Tokenize the texts\n",
    "encoding = tokenizer(test_texts, truncation=True, padding=True, max_length=216, return_tensors=\"pt\")\n",
    "\n",
    "# Get the input tensors\n",
    "input_ids = encoding['input_ids']\n",
    "attention_mask = encoding['attention_mask']\n",
    "\n",
    "# Make predictions\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient calculation during inference\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits  # Model outputs raw logits\n",
    "\n",
    "# Convert logits to predicted labels (using argmax to get the highest probability)\n",
    "predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Convert predictions and true labels to numpy arrays\n",
    "predicted_labels = predictions.numpy()\n",
    "true_labels = torch.tensor(true_labels).numpy()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "kVzVhxtJoZyV",
    "oQ04kvYaqxSl"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
